##### Class: Text Analytics and Natural Language Processing 
##### Karley Webster
##### MBAN2 HULT 2021
##### LDA Beta
##### Version 0.1


# There are two principles:
#1. Every document is a combination of multiple topics
#2. Every topic is a combination of multiple words

#install.packages("topicmodels")
library(topicmodels) #new articles from American news agencies
data("AssociatedPress")

AssociatedPress

#calling the Latent Dirichlet Allocation algorithm
ap_lda <- LDA(AssociatedPress, k=2, control=list(seed=123)) #"k" = number of topics you want your LDA model to have...
ap_lda

#now we are looking for the per topic per word probabilities aka. beta BETA MATRIX
#beta - what is the probability that "this term" will be generated by "this topic"
library(tidytext)
ap_topics <- tidy(ap_lda, matrix="beta")
ap_topics
library(ggplot2)
library(dplyr)
library(tidyr)

top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

#lets plot the term frequencies by topic
top_terms %>%
  mutate(term=reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) + #term = "x" variable and beta = "y variable"
  geom_col(show.legend = FALSE) + #Creating Bar Charts
  facet_wrap(~topic, scales = "free") + #Creating divisions depending on variables after the "~" sign
  coord_flip()

#lets calculate the relative difference between the betas for words in topic 1
#and words in topic 2

beta_spread <- ap_topics %>%
  mutate(topic=paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1>.001 | topic2 >0.001) %>%
  mutate(log_rate = log2(topic1/topic2))

beta_spread #The more positive the log_rate is = the strongest word related to Topic 1, while the more negative the log_rate is = the strongest word related to Topic 2#